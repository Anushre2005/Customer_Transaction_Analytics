{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Part A: Data Loading & Basic Operations\n",
        "\n",
        "Basic Exploration\n"
      ],
      "metadata": {
        "id": "hgsNEHlofaMx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Do6mW2mWfRZ_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "cdf=pd.read_csv('customers.csv')\n",
        "pdf=pd.read_csv('products.csv')\n",
        "tdf=pd.read_csv('transactions.csv')\n",
        "dfs=[cdf,pdf,tdf]\n",
        "for i, df in enumerate(dfs, 1):\n",
        "    print(f\"\\n{'='*20} Dataset {i} {'='*20}\")\n",
        "    print(\"Shape:\", df.shape)\n",
        "    print(\"Data Types:\\n\", df.dtypes)\n",
        "    print(\"First 3 Rows:\")\n",
        "    print(df.head(3))\n",
        "    print(\"Missing values (per column):\")\n",
        "    print(df.isnull().sum())\n",
        "num_duplicates = tdf['transaction_id'].duplicated().sum()\n",
        "print(f\"Number of duplicate transaction_id entries: {num_duplicates}\")\n",
        "dates = pd.to_datetime(tdf['timestamp'], errors='coerce')\n",
        "earliest = df['timestamp'].min()\n",
        "latest = df['timestamp'].max()\n",
        "\n",
        "print(f\"The date range is :{earliest} to {latest}\" )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Datetime Transformation"
      ],
      "metadata": {
        "id": "X3usDoB5ffJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df['timestamp'] = pd.to_datetime(tdf['timestamp'], errors='coerce')\n",
        "df['hour'] = df['timestamp'].dt.hour\n",
        "df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
        "df['month'] = df['timestamp'].dt.month\n",
        "print(df[['timestamp', 'hour', 'day_of_week', 'month']].head())"
      ],
      "metadata": {
        "id": "bPqzyVTCfXlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part B: Data Cleaning & Transformation\n",
        "\n",
        "Handle Missing Values"
      ],
      "metadata": {
        "id": "8i8ePzWXfkEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for col in tdf.select_dtypes(include='number').columns:\n",
        "    median_val = tdf[col].median()\n",
        "    tdf[col].fillna(median_val, inplace=True)\n",
        "\n",
        "tdf['payment_method'].fillna(tdf['payment_method'].mode, inplace=True)\n",
        "pdf['category'].fillna(pdf['category'].mode, inplace=True)\n",
        "\n",
        "cdf['age'].fillna(cdf['age'].mean, inplace=True)\n",
        "cdf['email'].fillna(cdf['email'].mode, inplace=True)"
      ],
      "metadata": {
        "id": "d-k5Bd-VfhEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tdf['revenue'] = tdf['quantity'] * tdf['price']\n",
        "full_data = tdf.merge(cdf, on='customer_id', how='left')\n",
        "full_data = full_data.merge(pdf, on='product_id', how='left')\n",
        "full_data['profit_margin'] = ((full_data['price'] - full_data['cost_price']) / full_data['price']) * 100\n",
        "\n",
        "full_data.head(5)\n",
        "\n"
      ],
      "metadata": {
        "id": "g2N2cmCWfmcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part C: Aggregation & Analysis\n",
        "\n",
        "C1. Customer Metrics\n"
      ],
      "metadata": {
        "id": "5Iq356d7gws9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def most_frequent_category(series):\n",
        "    return series.mode().iat[0]\n",
        "customer_summary = full_data.groupby('customer_id').agg(\n",
        "    total_revenue=pd.NamedAgg(column='revenue', aggfunc='sum'),\n",
        "    num_transactions=pd.NamedAgg(column='transaction_id', aggfunc='count'),\n",
        "    avg_transaction_value=pd.NamedAgg(column='revenue', aggfunc='mean'),\n",
        "    most_freq_category=pd.NamedAgg(column='category', aggfunc=most_frequent_category)\n",
        ")\n",
        "\n",
        "customer_summary = customer_summary.sort_values(by='total_revenue', ascending=False)\n",
        "\n",
        "print(customer_summary.head())"
      ],
      "metadata": {
        "id": "hx4iorY7gIMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "C2. Time-Based Analysis\n"
      ],
      "metadata": {
        "id": "qlMFrZIOmbH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def most_frequent_category(series):\n",
        "    return series.mode().iat[0]\n",
        "monthly_summary = full_data.groupby('month').agg(\n",
        "    total_revenue=pd.NamedAgg(column='revenue', aggfunc='sum'),\n",
        "    num_unique_customers=pd.NamedAgg(column='customer_id', aggfunc='nunique'),\n",
        "    avg_order_value=pd.NamedAgg(column='revenue', aggfunc='mean'),\n",
        ").reset_index()\n",
        "\n",
        "monthly_summary['mom_growth_pct'] = monthly_summary['total_revenue'].pct_change() * 100\n",
        "\n",
        "print(customer_summary.head())\n"
      ],
      "metadata": {
        "id": "XvnLTPP1lhVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Product performance\n"
      ],
      "metadata": {
        "id": "7oZXSEy0ogwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "product_perf = full_data.groupby('product_id').agg(\n",
        "    total_revenue=pd.NamedAgg(column='revenue', aggfunc='sum'),\n",
        "    total_quantity=pd.NamedAgg(column='quantity', aggfunc='sum'),\n",
        "    avg_profit_margin=pd.NamedAgg(column='profit_margin', aggfunc='mean')\n",
        ").reset_index()\n",
        "\n",
        "top10_revenue = product_perf.nlargest(10, 'total_revenue')\n",
        "top10_quantity = product_perf.nlargest(10, 'total_quantity')\n",
        "top10_profit_margin = product_perf.nlargest(10, 'avg_profit_margin')\n",
        "\n",
        "print(\"Top 10 Products by Total Revenue:\")\n",
        "print(top10_revenue)\n",
        "\n",
        "print(\"\\nTop 10 Products by Total Quantity Sold:\")\n",
        "print(top10_quantity)\n",
        "\n",
        "print(\"\\nTop 10 Products by Average Profit Margin:\")\n",
        "print(top10_profit_margin)"
      ],
      "metadata": {
        "id": "p9K0imx5nwSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latest_date = full_data['timestamp'].max()\n",
        "\n",
        "rfm = full_data.groupby('customer_id').agg(\n",
        "    recency=pd.NamedAgg(column='timestamp', aggfunc=lambda x: (latest_date - x.max()).days),\n",
        "    frequency=('transaction_id', 'count'),\n",
        "    monetary=('revenue', 'sum')\n",
        ").reset_index()\n",
        "\n",
        "bins = [0, 0.33, 0.66, 1]\n",
        "labels = ['Low', 'Medium', 'High']\n",
        "\n",
        "rfm['recency_bin'] = pd.qcut(rfm['recency'], q=bins, labels=labels, duplicates='drop').astype(str)\n",
        "rfm['frequency_bin'] = pd.qcut(rfm['frequency'], q=bins, labels=labels, duplicates='drop').astype(str)\n",
        "rfm['monetary_bin'] = pd.qcut(rfm['monetary'], q=bins, labels=labels, duplicates='drop').astype(str)\n",
        "\n",
        "rfm['segment'] = rfm['recency_bin'] + '-' + rfm['frequency_bin'] + '-' + rfm['monetary_bin']\n",
        "\n",
        "print(\"\\nCustomer RFM Segmentation Sample\")\n",
        "print(rfm.head())"
      ],
      "metadata": {
        "id": "VEDtLPoAokan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part D: Advanced Operations\n",
        "\n",
        "Efficient Operations Challenge\n"
      ],
      "metadata": {
        "id": "UW7KPSbcrLZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_suspicious_transactions(full_data):\n",
        "    \"\"\"\n",
        "    Identify suspicious transactions using vectorized operations.\n",
        "    Returns: DataFrame with columns [transaction_id, customer_name, reason]\n",
        "    \"\"\"\n",
        "\n",
        "    cond1 = (full_data['quantity'] > 100) & (full_data['price'] < 10)\n",
        "\n",
        "    full_data['hour'] = full_data['timestamp'].dt.floor('H')\n",
        "    purchase_counts = full_data.groupby(['customer_id', 'hour'])['transaction_id'].transform('count')\n",
        "    cond2 = purchase_counts > 3\n",
        "\n",
        "\n",
        "    suspicious_mask = cond1 | cond2\n",
        "\n",
        "    full_data['reason'] = ''\n",
        "\n",
        "    full_data.loc[cond1, 'reason'] = 'Qty > 100 and Price < 10'\n",
        "    full_data.loc[cond2, 'reason'] = full_data.loc[cond2, 'reason'] + '; More than 3 purchases '\n",
        "\n",
        "    result = full_data.loc[suspicious_mask, ['transaction_id', 'customer_name', 'reason']].copy()\n",
        "\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "19E6d8R8qcVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rolling Window Analysis\n"
      ],
      "metadata": {
        "id": "RrX_KrHRsNVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Normalize columns names\n",
        "full_data.columns = full_data.columns.str.strip().str.lower()\n",
        "\n",
        "# Aggregate daily revenue (timestamp to date)\n",
        "full_data['date'] = full_data['timestamp'].dt.date\n",
        "daily_revenue = full_data.groupby('date')['revenue'].sum().reset_index()\n",
        "\n",
        "# Convert date to datetime type for rolling calculations\n",
        "daily_revenue['date'] = pd.to_datetime(daily_revenue['date'])\n",
        "\n",
        "# Calculate 7-day moving average of daily revenue with min_periods=1 to handle start edges\n",
        "daily_revenue['7_day_revenue'] = daily_revenue['revenue'].rolling(window=7, min_periods=1).mean()\n",
        "\n",
        "# Feature engineering for customers\n",
        "\n",
        "# Convert signup_date to datetime\n",
        "full_data['signup_date'] = pd.to_datetime(full_data['signup_date'])\n",
        "\n",
        "# Get first transaction date per customer\n",
        "first_transaction_date = full_data.groupby('customer_id')['timestamp'].min().reset_index()\n",
        "first_transaction_date.rename(columns={'timestamp': 'first_transaction_date'}, inplace=True)\n",
        "\n",
        "# Build initial customer features dataframe\n",
        "customer_features = full_data[['customer_id', 'category', 'payment_method']].drop_duplicates(subset=['customer_id', 'category', 'payment_method'])\n",
        "customer_features = customer_features.merge(first_transaction_date, on='customer_id')\n",
        "customer_features = customer_features.merge(full_data[['customer_id', 'signup_date']].drop_duplicates(), on='customer_id')\n",
        "\n",
        "# Calculate days_since_signup for each customer\n",
        "customer_features['days_since_signup'] = (customer_features['first_transaction_date'] - customer_features['signup_date']).dt.days\n",
        "\n",
        "# Calculate purchase_frequency: average days between purchases\n",
        "purchase_freq = full_data.groupby('customer_id')['timestamp'].apply(lambda x: x.sort_values().diff().dt.days.mean()).reset_index()\n",
        "purchase_freq.rename(columns={'timestamp': 'purchase_frequency'}, inplace=True)\n",
        "\n",
        "customer_features = customer_features.merge(purchase_freq, on='customer_id')\n",
        "\n",
        "# Category diversity: count unique categories per customer\n",
        "category_diversity = full_data.groupby('customer_id')['category'].nunique().reset_index()\n",
        "category_diversity.rename(columns={'category': 'category_diversity'}, inplace=True)\n",
        "customer_features = customer_features.merge(category_diversity, on='customer_id')\n",
        "\n",
        "# Preferred payment method\n",
        "preferred_payment = full_data.groupby('customer_id')['payment_method'].agg(lambda x: x.mode().iat[0]).reset_index()\n",
        "preferred_payment.rename(columns={'payment_method': 'preferred_payment'}, inplace=True)\n",
        "customer_features = customer_features.merge(preferred_payment, on='customer_id')\n",
        "\n",
        "# Display output samples\n",
        "print(daily_revenue.head())\n",
        "print(customer_features.head())\n"
      ],
      "metadata": {
        "id": "J8-5x6J5re8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part E: Code Quality & Efficiency\n",
        "\n",
        "E1. Optimization\n"
      ],
      "metadata": {
        "id": "QGsg62RPt29s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def customer_lifetime_value(transactions_df, customer_id, discount_rate=0.1):\n",
        "    \"\"\"\n",
        "    Calculate the lifetime value of a customer with monthly discounting.\n",
        "\n",
        "    CLV = sum of (revenue * (1 - discount_rate) ^ month_from_first_purchase)\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    transactions_df : pd.DataFrame\n",
        "        Should contain 'customer_id', 'timestamp' (datetime), and 'revenue' columns.\n",
        "    customer_id : int\n",
        "        Customer ID you want to calculate CLV for.\n",
        "    discount_rate : float\n",
        "        Monthly discount rate (default 0.1)\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    float\n",
        "        The lifetime value calculated for the customer.\n",
        "    \"\"\"\n",
        "    customer_data = transactions_df[transactions_df['customer_id'] == customer_id].copy()\n",
        "    if customer_data.empty:\n",
        "        return 0.0\n",
        "\n",
        "    customer_data['timestamp'] = pd.to_datetime(customer_data['timestamp'])\n",
        "\n",
        "    first_month = customer_data['timestamp'].min().to_period('M')\n",
        "\n",
        "    customer_data['months_since_first'] = customer_data['timestamp'].dt.to_period('M').apply(lambda x: x.ordinal - first_month.ordinal)\n",
        "\n",
        "    customer_data['discount_factor'] = (1 - discount_rate) ** customer_data['months_since_first']\n",
        "\n",
        "    customer_data['discounted_revenue'] = customer_data['revenue'] * customer_data['discount_factor']\n",
        "\n",
        "    clv = customer_data['discounted_revenue'].sum()\n",
        "\n",
        "    return clv\n"
      ],
      "metadata": {
        "id": "xairxQQStOXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8GAcUuaCuJmp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}